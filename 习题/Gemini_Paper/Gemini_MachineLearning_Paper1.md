### 第一部分：期末高频考点总结 (划重点)

根据《25-26机器学习复习知识点.md》及历年习题，以下内容极大概率出现在试卷中：

**1. 核心概念 (Ch1-Ch2)**

* **过拟合 vs 欠拟合** ：定义、表现（方差vs偏差）、缓解方法（正则化、剪枝、Dropout等）。
* **评估指标** ：查准率(P)、查全率(R)、F1值、ROC/AUC含义、混淆矩阵计算。
* **数据划分** ：留出法、交叉验证法（k折）、自助法（Bootstrap）的区别。

**2. 经典模型原理 (Ch3-Ch6)**

* **线性模型** ：逻辑回归（虽然叫回归但是用于分类，Sigmoid函数性质）。
* **决策树** ：三大划分准则的区别（ID3-信息增益、C4.5-增益率、CART-基尼指数）；剪枝（预剪枝vs后剪枝）；停止条件（P74）。
* **SVM** ：支持向量的定义（决定超平面的点）；核技巧的作用（低维映射高维解决线性不可分）；软间隔与正则化。

**3. 概率与集成 (Ch7-Ch8)**

* **贝叶斯** ：朴素贝叶斯的“属性条件独立性假设”；拉普拉斯平滑；后验概率计算。
* **集成学习** ：Bagging（随机森林，降低方差，并行）vs Boosting（AdaBoost，降低偏差，串行）。

**4. 无监督学习 (Ch9-Ch10)**

* **聚类** ：K-Means算法流程；DBSCAN（密度聚类）流程；聚类性能指标。
* **降维** ：PCA（主成分分析）的核心思想；kNN（懒惰学习）。

---

### 第二部分：2025-2026 机器学习期末模拟试卷

 **说明** ：本试卷严格参照《25-26机器学习复习知识点.md》的题型与分值分布。

* **适用对象** ：河南大学计算机学院学生
* **考试时长** ：120分钟

#### 一、选择题 (共10题，每题2分，共20分)

1. 机器学习中，若有多个假设与观察一致，则选最简单的那个，这被称为（ ）。
   A. 没有免费午餐定理
   B. 丑小鸭定理
   C. 奥卡姆剃刀原则
   D. 极大似然估计
2. 下列关于“过拟合”与“欠拟合”的说法，正确的是（ ）。
   A. 欠拟合通常对应较低的偏差（Bias）
   B. 过拟合通常对应较高的方差（Variance）
   C. 决策树增加深度可以缓解过拟合
   D. 增加训练数据量会导致过拟合
3. 在二分类问题中，如果将阈值从0.5提高到0.9（大于阈值判为正例），通常会导致（ ）。
   A. 查准率（Precision）下降，查全率（Recall）上升
   B. 查准率（Precision）上升，查全率（Recall）下降
   C. 两者都上升
   D. 两者都下降
4. ID3决策树算法在选择划分属性时，依据的指标是（ ）。
   A. 信息增益
   B. 增益率
   C. 基尼指数
   D. 对数似然
5. 逻辑回归（Logistic Regression）常用的激活函数是（ ），其值域为（0,1）。
   A. ReLU
   B. Sigmoid
   C. Tanh
   D. Softmax
6. SVM（支持向量机）在解决非线性可分问题时，引入了（ ）技术将样本映射到高维空间。
   A. 软间隔
   B. 正则化
   C. 核函数
   D. 梯度下降
7. 朴素贝叶斯分类器的“朴素”是指（ ）。
   A. 假设所有特征对分类结果影响相同
   B. 假设所有特征之间相互独立
   C. 算法逻辑简单
   D. 不需要训练过程
8. 关于集成学习，下列说法错误的是（ ）。
   A. 随机森林属于Bagging方法
   B. AdaBoost属于Boosting方法
   C. Bagging主要关注降低方差
   D. Boosting各基学习器之间是独立的，可以并行训练
9. K-Means聚类算法属于（ ）。
   A. 密度聚类
   B. 层次聚类
   C. 原型聚类
   D. 谱聚类
10. 下列关于k近邻（kNN）算法的说法，错误的是（ ）。
    A. kNN是一种懒惰学习（Lazy Learning）算法
    B. k值的选择对结果影响很大
    C. kNN在训练阶段开销很大
    D. 距离度量通常使用欧氏距离

---

#### 二、判断题 (共10题，每题2分，共20分)

1. ( ) 偏差（Bias）度量了学习算法的期望预测与真实结果的偏离程度，刻画了算法本身的拟合能力。
2. ( ) 在神经网络中，如果隐藏层和输出层都不使用激活函数，无论网络多深，其本质上仍是一个线性模型。
3. ( ) 决策树的预剪枝优于后剪枝，因为它保留了更多的分支。
4. ( ) SVM中的支持向量是指那些距离超平面最远的样本点。
5. ( ) 训练误差为0的模型一定是一个好模型。
6. ( ) 朴素贝叶斯算法中，如果某个属性值在训练集中没有出现过，计算概率时会为0，可以使用拉普拉斯平滑解决。
7. ( ) 随机森林中，基学习器的多样性仅来自于样本的随机采样（Bootstrap）。
8. ( ) K-Means算法对初始中心点的选择敏感，不同的初始点可能导致不同的聚类结果。
9. ( ) 降维（如PCA）属于监督学习的一种。
10. ( ) 留出法中，测试集可以直接用于调整模型的超参数。

---

#### 三、简答题 (共4题，共30分)

1. **（7分）什么是过拟合（Overfitting）？请列举至少三种缓解过拟合的方法。**
2. **（8分）请简述决策树生成的三个主要阶段（特征选择、树生成、剪枝），并说明“预剪枝”和“后剪枝”的区别。**
3. **（7分）简述支持向量机（SVM）中“软间隔”与“硬间隔”的区别，并说明引入核函数（Kernel Trick）的作用。**
4. **（8分）请简述集成学习中Bagging和Boosting的主要区别（从样本选择、样本权重、能否并行、偏差/方差角度）。**

---

#### 四、计算题 (共2题，共30分)

**1. 性能度量计算（15分）**

在一个二分类任务中，模型在测试集上的预测结果如下表所示（1为正例，0为反例）：

| **样本编号** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** | **10** |
| ------------------ | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ------------ |
| **真实标签** | 1           | 1           | 1           | 1           | 1           | 0           | 0           | 0           | 0           | 0            |
| **预测标签** | 1           | 1           | 1           | 0           | 0           | 1           | 0           | 0           | 0           | 0            |

请计算：

(1) 写出混淆矩阵（TP, FN, FP, TN 的值）。

(2) 计算查准率（Precision）。

(3) 计算查全率（Recall）。

(4) 计算F1度量值（保留两位小数）。

**2. 朴素贝叶斯计算（15分）**

假设有一个数据集如下，我们要预测一个新的样本 **$X_{new} = (\text{颜色=青绿}, \text{敲声=浊响})$** 是否是“好瓜”。

**训练集数据：**

| **编号** | **颜色** | **敲声** | **好瓜 (类别)** |
| -------------- | -------------- | -------------- | --------------------- |
| 1              | 青绿           | 浊响           | 是                    |
| 2              | 乌黑           | 浊响           | 是                    |
| 3              | 青绿           | 清脆           | 否                    |
| 4              | 乌黑           | 沉闷           | 否                    |
| 5              | 青绿           | 浊响           | 是                    |
| 6              | 浅白           | 清脆           | 否                    |

(1) 计算先验概率 $P(\text{好瓜}=\text{是})$ 和 $P(\text{好瓜}=\text{否})$。

(2) 计算类条件概率：

* $P(\text{颜色=青绿} | \text{好瓜}=\text{是})$
* $P(\text{敲声=浊响} | \text{好瓜}=\text{是})$
* $P(\text{颜色=青绿} | \text{好瓜}=\text{否})$
* $P(\text{敲声=浊响} | \text{好瓜}=\text{否})$

(3) 利用朴素贝叶斯公式，判断 $X_{new}$ 是好瓜还是坏瓜（需写出比较过程）。

(注：为简化计算，不使用拉普拉斯平滑)

---

---

### 参考答案

#### 一、选择题

1-5: C B B A B

6-10: C B D C C

#### 二、判断题

1. √ (偏差刻画拟合能力)
2. √ (无激活函数即线性变换的叠加)
3. × (后剪枝通常泛化性能更好，预剪枝可能导致欠拟合)
4. × (支持向量是距离超平面最近的点)
5. × (可能过拟合)
6. √ (拉普拉斯平滑作用)
7. × (随机森林还有属性选择的随机性)
8. √ (K-Means局部最优)
9. × (PCA是无监督学习)
10. × (测试集只能最后用一次，验证集才用于调参)

#### 三、简答题（要点）

1. 过拟合：模型在训练集表现很好，但在测试集表现很差（泛化能力弱）。
   方法：
   1. 增加数据量。
   2. 正则化（L1/L2）。
   3. 降低模型复杂度（如减少神经网络层数、决策树剪枝）。
   4. Dropout（神经网络）。
   5. 提前停止训练（Early Stopping）。
2. 阶段：特征选择（选最优划分属性）、决策树生成（递归生成节点）、剪枝（处理过拟合）。
   区别：
   * **预剪枝** ：在生成过程中，对每个节点在划分前进行估计，若划分不能带来性能提升则停止。特点：速度快，但有欠拟合风险。
   * **后剪枝** ：先生成完整树，再自底向上考察非叶节点，若替换为叶节点能提升性能则剪枝。特点：泛化性能好，但计算开销大。
3. **软硬间隔** ：

* 硬间隔：要求所有样本都严格满足分类约束（不允许出错），仅适用于线性可分数据。
* 软间隔：允许部分样本不满足约束（允许少量分类错误），引入松弛变量。
* **核函数作用** ：将原始低维空间中线性不可分的样本映射到高维特征空间，使其在高维空间线性可分。

1. **Bagging vs Boosting** ：

* **样本选择** ：Bagging有放回采样（Bootstrap）；Boosting使用全部数据但调整权重。
* **样本权重** ：Bagging均匀；Boosting错误样本权重高。
* **并行** ：Bagging各基学习器独立，可并行；Boosting必须串行。
* **偏差/方差** ：Bagging主要降低方差（抗噪）；Boosting主要降低偏差（提升精度）。

#### 四、计算题

1. **性能度量**
   * TP=3 (样本1,2,3), FN=2 (样本4,5), FP=1 (样本6), TN=4 (样本7,8,9,10)
   * Precision = TP / (TP + FP) = 3 / (3+1) = 0.75
   * Recall = TP / (TP + FN) = 3 / (3+2) = 0.6
   * F1 = (2 * P * R) / (P + R) = (2 * 0.75 * 0.6) / (1.35) ≈ 0.67
2. **朴素贝叶斯**
   * 总样本6个。是=3个，否=3个。
   * (1) **$P(\text{是}) = 3/6 = 0.5$**; **$P(\text{否}) = 3/6 = 0.5$**
   * (2)
     * **$P(\text{青绿}|\text{是}) = 2/3$** (样本1,5)
     * **$P(\text{浊响}|\text{是}) = 3/3 = 1$** (样本1,2,5)
     * **$P(\text{青绿}|\text{否}) = 1/3$** (样本3)
     * **$P(\text{浊响}|\text{否}) = 0/3 = 0$** (样本中没有坏瓜且浊响的)
   * (3)
     * **$P(\text{是}|X) \propto P(\text{是}) \times P(\text{青绿}|\text{是}) \times P(\text{浊响}|\text{是}) = 0.5 \times (2/3) \times 1 \approx 0.333$**
     * **$P(\text{否}|X) \propto P(\text{否}) \times P(\text{青绿}|\text{否}) \times P(\text{浊响}|\text{否}) = 0.5 \times (1/3) \times 0 = 0$**
   * **结论** ：**$0.333 > 0$**，所以预测为 **好瓜** 。
