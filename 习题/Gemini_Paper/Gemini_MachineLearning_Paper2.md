### 第一部分：核心考点预测（必背知识点）

根据你的资料，考试范围集中在第1-9章，以下是高频考点：

1. **绪论 & 模型评估**

   * **泛化能力**：模型对新样本的适应能力。
   * **过拟合/欠拟合**：过拟合（训练误差低，测试误差高，高方差），欠拟合（训练误差高，高偏差）。
   * **评估指标**：查准率（Precision）、查全率（Recall）、F1值、ROC曲线与AUC（物理意义）、混淆矩阵。
   * **验证方法**：留出法、交叉验证法（k折）、自助法（适合数据集小）。
2. **线性模型**

   * **逻辑回归（Logistic Regression）**：虽然叫回归，但是是**分类**模型。激活函数是Sigmoid，损失函数是对数似然（交叉熵）。
   * **最小二乘法**：线性回归的参数求解方法。
3. **决策树**

   * **划分准则**：ID3（信息增益，偏好取值多的属性）、C4.5（增益率，偏好取值少的属性）、CART（基尼指数，二叉树）。
   * **剪枝**：预剪枝（容易欠拟合，速度快）、后剪枝（容易过拟合，泛化好，慢）。
4. **神经网络**

   * **BP算法**：基于梯度下降策略，利用链式法则反向传播误差。
   * **激活函数**：Sigmoid（易梯度消失）、ReLU（常用）。
5. **支持向量机 (SVM)**

   * **核心思想**：最大化间隔（Margin）。
   * **支持向量**：决定超平面的只有支持向量，其他样本删除也不影响模型。
   * **核函数**：处理线性不可分问题，映射到高维空间。
6. **贝叶斯分类器**

   * **朴素贝叶斯假设**：属性条件独立性假设。
   * **拉普拉斯平滑**：解决零概率问题（分子+1，分母+N）。
7. **集成学习**

   * **Bagging**：并行，减少**方差**，代表是随机森林（样本随机+属性随机）。
   * **Boosting**：串行，减少**偏差**，代表是AdaBoost。
8. **聚类**

   * **K-Means**：原型聚类，需预设K值，受初始值影响大。
   * **距离计算**：欧氏距离。

---

### 第二部分：2025-2026 机器学习期末模拟试卷

**试卷说明：**

* **总分**：100分
* **题型**：按照“25-26机器学习复习知识点.md”要求设置。

#### 一、选择题（每题2分，共20分）

1. **（基础概念）** 机器学习中，关于“归纳偏好”的说法错误的是（ ）。
   A. 任何一个有效的机器学习算法都有其归纳偏好
   B. “奥卡姆剃刀”原则是常见的归纳偏好之一
   C. 脱离具体问题，空泛地谈论“在这个算法比那个算法好”是没有意义的
   D. 归纳偏好必然导致模型产生过拟合
2. **（模型评估）** 在P-R曲线（查准率-查全率曲线）中，如果学习器A的曲线完全包住了学习器B的曲线，则（ ）。
   A. 学习器A的性能优于B
   B. 学习器B的性能优于A
   C. 两者性能相同
   D. 无法比较
3. **（线性模型）** 对数几率回归（Logistic Regression）主要用于解决（ ）问题。
   A. 线性回归
   B. 二分类
   C. 聚类
   D. 降维
4. **（决策树）** ID3算法、C4.5算法和CART算法分别使用（ ）作为属性划分准则。
   A. 信息增益、基尼指数、增益率
   B. 信息增益、增益率、基尼指数
   C. 基尼指数、信息增益、增益率
   D. 增益率、信息增益、基尼指数
5. **（SVM）** 在支持向量机中，如果移除（ ），最终的决策超平面（模型）不会发生改变。
   A. 所有的支持向量
   B. 所有的非支持向量
   C. 任意一个训练样本
   D. 距离超平面最近的样本
6. **（神经网络）** 训练神经网络时，如果训练误差很低，但测试误差很高，这通常意味着模型出现了（ ），可以通过（ ）来缓解。
   A. 欠拟合；增加网络层数
   B. 过拟合；引入正则化或Dropout
   C. 欠拟合；引入正则化
   D. 过拟合；增加训练轮数
7. **（贝叶斯）** 朴素贝叶斯分类器的“朴素”指的是（ ）。
   A. 算法逻辑简单
   B. 假设所有属性对分类结果的影响是独立的
   C. 计算过程不需要求逆矩阵
   D. 只能处理离散属性
8. **（集成学习）** 随机森林（Random Forest）属于（ ）集成方法，它通过引入（ ）来增加模型的多样性。
   A. Boosting；样本权重
   B. Bagging；属性选择的随机性
   C. Stacking；层级结构
   D. Bagging；残差拟合
9. **（聚类）** 关于K-Means聚类算法，下列说法错误的是（ ）。
   A. 需要预先指定聚类簇数K
   B. 算法对初始聚类中心的选择敏感
   C. 每一轮迭代都能保证找到全局最优解
   D. 采用欧氏距离作为相似度度量
10. **（模型选择）** 在k折交叉验证中，假设k=10，则每次训练使用的样本比例是（ ）。
    A. 10%
    B. 50%
    C. 90%
    D. 100%

#### 二、判断题（每题2分，共20分，对的打√，错的打×）

1. （ ）机器学习中，无监督学习的训练数据不包含标记信息（Label）。
2. （ ）在分类任务中，查准率（Precision）和查全率（Recall）通常是一对矛盾的度量，一个高另一个往往偏低。
3. （ ）决策树的预剪枝优于后剪枝，因为预剪枝可以保留更多的分支，泛化能力更强。
4. （ ）逻辑回归（Logistic Regression）的输出值范围是(-∞, +∞)。
5. （ ）SVM中的核函数（Kernel Function）的作用是将低维线性不可分的数据映射到高维空间，使其线性可分。
6. （ ）BP神经网络算法是基于梯度下降策略来调整连接权重的。
7. （ ）集成学习中，Boosting主要关注降低偏差（Bias），而Bagging主要关注降低方差（Variance）。
8. （ ）K-Means算法能处理任意形状的聚类簇（如环形）。
9. （ ）自助法（Bootstrap）在数据集较小难以有效划分训练/测试集时很有用，但会改变数据的分布。
10. （ ）L2正则化可以通过让权重系数w变大来防止过拟合。

#### 三、简答题（每题7.5分，共30分）

1. **（模型评估）** 什么是过拟合（Overfitting）和欠拟合（Underfitting）？请分别列举一种常见的缓解过拟合的方法。
2. **（SVM）** 请简述支持向量机（SVM）中“支持向量”的概念，并解释为什么SVM具有较好的鲁棒性。
3. **（集成学习）** 简述Bagging和Boosting的主要区别（从训练样本选取、样本权重、学习器关系三个角度回答）。
4. **（聚类）** 请简述K-Means聚类算法的基本流程。

#### 四、计算题（每题15分，共30分）

**1. 朴素贝叶斯计算（含拉普拉斯修正）**

假设有一个用于判断“是否适合打球”的训练数据集如下表所示：

| 编号 | 天气 | 温度 | 湿度 | 是否适合打球 (Y) |
| :--- | :--- | :--- | :--- | :--------------- |
| 1    | 晴   | 高   | 高   | 否               |
| 2    | 晴   | 高   | 高   | 否               |
| 3    | 阴   | 高   | 高   | 是               |
| 4    | 雨   | 中   | 高   | 是               |
| 5    | 雨   | 低   | 正常 | 是               |
| 6    | 阴   | 低   | 正常 | 否               |

**问题：**
现有一个新样本 **X = {天气=晴, 温度=低, 湿度=正常}**。
请使用**朴素贝叶斯分类器**判断该样本的类别（是/否）。
**要求**：计算过程中使用**拉普拉斯修正**（即分子加1，分母加类别数Ni）。

*提示：
先验概率公式：$P(c) = \frac{|D_c| + 1}{|D| + N}$
条件概率公式：$P(x_i|c) = \frac{|D_{c,x_i}| + 1}{|D_c| + N_i}$*

---

**2. K-Means 聚类计算**

假设要在 一维 空间对以下5个点进行K-Means聚类，设 K=2。
数据点坐标为：**{2, 4, 10, 12, 3}**

**第一轮初始化：**
假设随机选择的两个初始聚类中心为：$\mu_1 = 2$, $\mu_2 = 10$。

**问题：**

1. 请计算第一轮迭代后，各样本点分别属于哪个簇（Cluster 1 或 Cluster 2）。
2. 请计算第一轮迭代结束后的**新聚类中心**（均值）$\mu_1'$ 和 $\mu_2'$ 是多少。
3. （简答）如果第二轮计算出的中心不再变化，说明了什么？

---

### 第三部分：参考答案与解析

#### 一、选择题答案

1. **D** （归纳偏好是必须的，不一定导致过拟合，没有偏好就无法学习）
2. **A** （P-R曲线全包围说明性能更优）
3. **B** （名字叫回归，实为分类）
4. **B** （ID3-增益，C4.5-增益率，CART-基尼）
5. **B** （非支持向量对超平面无影响）
6. **B** （这是典型的过拟合，需正则化）
7. **B** （属性条件独立性假设）
8. **B** （随机森林是Bagging的扩展，引入属性随机）
9. **C** （K-Means容易陷入局部最优，不保证全局最优）
10. **C** （k折交叉验证，k-1份训练，1份测试，故9/10）

#### 二、判断题答案

1. **√**
2. **√**
3. **×** （后剪枝通常泛化能力更好，但开销大；预剪枝容易欠拟合）
4. **×** （LogReg经过Sigmoid后输出为0~1的概率值）
5. **√**
6. **√**
7. **√** （Boosting-偏差，Bagging-方差）
8. **×** （K-Means基于欧氏距离，适合凸形/球形簇，处理不了环形）
9. **√**
10. **×** （L2正则化让权重w变小/趋向于0，而不是变大）

#### 三、简答题答案

1. **过拟合与欠拟合：**

   * **欠拟合**：模型学习能力太差，没学好训练样本的一般性质（训练误差大）。解决方法：增加特征、增加模型复杂度（如神经网络增加层数）、减小正则化参数。
   * **过拟合**：模型学习能力太强，把训练样本的噪声也学了，导致在测试集表现差（训练误差小，测试误差大）。解决方法：**正则化（L1/L2）**、**早停（Early Stopping）**、**Dropout**、增加训练数据。
2. **SVM支持向量与鲁棒性：**

   * **概念**：支持向量是距离超平面最近的那些训练样本点。只有这些点决定了超平面的位置，满足 $y_i(w^Tx_i+b) = 1$。
   * **鲁棒性**：因为超平面仅由少数支持向量决定，剔除或移动非支持向量的样本不会改变模型。这使得SVM对非支持向量的噪声数据不敏感，具有较好的鲁棒性。
3. **Bagging与Boosting区别：**

   * **样本选取**：Bagging使用自助采样（有放回），各训练集独立；Boosting每一轮的训练集依赖上一轮，增加错误样本的权重。
   * **样本权重**：Bagging中样本权重相等；Boosting中分类错误的样本权重会增大。
   * **学习器关系**：Bagging是个体独立并行生成；Boosting是个体串行生成，强依赖。
4. **K-Means流程：**

   1. 随机选择 $K$ 个样本作为初始均值向量（聚类中心）。
   2. 计算所有样本到各聚类中心的距离，将样本划分到距离最近的簇。
   3. 根据划分结果，重新计算每个簇的新均值向量（中心）。
   4. 重复步骤2和3，直到聚类中心不再变化或达到最大迭代次数。

#### 四、计算题答案

**1. 朴素贝叶斯计算**

* **先验概率 P(c)**：

  * 总样本 |D|=6，类别数 N=2（是/否）。
  * P(是) = (3+1) / (6+2) = 4/8 = 0.5
  * P(否) = (3+1) / (6+2) = 4/8 = 0.5
* **条件概率 P(x|c)**：

  * **对于 P(x|是)**：(类别为"是"的样本有3个: 3,4,5)
    * P(晴|是) = (0+1) / (3+3) = 1/6  *(天气有晴阴雨3种状态，Ni=3)*
    * P(低|是) = (1+1) / (3+2) = 2/5  *(温度有高中低3种，注意：通常若按离散算Ni=3，若题目只有高低则Ni=2，这里假设高中低，Ni=3。修正：题目中温度出现高、中、低，故Ni=3)* -> P(低|是) = (1+1)/(3+3)=2/6=1/3
    * P(正常|是) = (1+1) / (3+2) = 2/5 *(湿度有高、正常，Ni=2)*
  * **对于 P(x|否)**：(类别为"否"的样本有3个: 1,2,6)
    * P(晴|否) = (2+1) / (3+3) = 3/6 = 0.5
    * P(低|否) = (1+1) / (3+3) = 2/6 = 1/3
    * P(正常|否) = (1+1) / (3+2) = 2/5
* **计算后验概率 P(c|X) $\propto$ P(c) * Π P(xi|c)**：

  * **P(是|X)** $\propto$ 0.5 * (1/6) * (1/3) * (2/5) = 0.5 * 0.166 * 0.333 * 0.4 ≈ **0.0111**
  * **P(否|X)** $\propto$ 0.5 * 0.5 * (1/3) * (2/5) = 0.5 * 0.5 * 0.333 * 0.4 ≈ **0.0333**
* **结论**：
  因为 0.0333 > 0.0111，所以预测该样本类别为 **“否”**（不适合打球）。

**2. K-Means 计算**

* 数据：{2, 3, 4, 10, 12} (排序后方便看)，初始中心 $\mu_1=2, \mu_2=10$。
* **Step 1: 划分簇**

  * 点 2：距离 $\mu_1(2)$ 为 0，距离 $\mu_2(10)$ 为 8 -> 归属 **簇1**。
  * 点 4：距离 $\mu_1(2)$ 为 2，距离 $\mu_2(10)$ 为 6 -> 归属 **簇1**。
  * 点 10：距离 $\mu_1(2)$ 为 8，距离 $\mu_2(10)$ 为 0 -> 归属 **簇2**。
  * 点 12：距离 $\mu_1(2)$ 为 10，距离 $\mu_2(10)$ 为 2 -> 归属 **簇2**。
  * 点 3：距离 $\mu_1(2)$ 为 1，距离 $\mu_2(10)$ 为 7 -> 归属 **簇1**。
  * **簇划分结果**：

    * Cluster 1: {2, 3, 4}
    * Cluster 2: {10, 12}
* **Step 2: 更新中心**

  * $\mu_1' = (2 + 3 + 4) / 3 = 9 / 3 = \mathbf{3}$
  * $\mu_2' = (10 + 12) / 2 = 22 / 2 = \mathbf{11}$
* **Step 3: 简答**

  * 如果中心不再变化，说明算法已经**收敛**（Converged），找到了当前的局部最优解，聚类过程结束。
