1、如何定义机器学习？流程

 机器学习是指从有限的观测数据中学习(或“猜测”) 出具有一般性的规律（一般指数学关系），并利用这些规律对未知数据进行预测的方法。

流程：数据收集与预处理，模型选择，模型训练，模型评估，模型调优，模型部署

解释机器学习术语：什么是特征，什么是标签。

特征：指描述数据样本的属性或变量，是模型输入的独立变量。

标签：指模型需要预测的目标值或输出变量，通常对应监督学习中的因变量。

最常见的两种监督学习任务是什么？

分类和回归。

7、查准率和查全率是分类任务中常用的性能度量指标，请写出其公式并对这两种指标进行分析。

答：查准率P=TP/（TP+FP）

    查准率：关注模型预测的准确性，即预测为正例的样本有多少是真正的正例

查全率R=TP/（TP+FN）

查全率：关注模型的正例完整性，即正例中有多少被成功识别

8. 简述k折交叉验证法。

答：“交叉验证法”先将数据集D划分为个大小相似的互斥子集,每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到，然后，每次用

K-1个子集的并集作为训练集，余下的那个子集作测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这个测试结果的均值.显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为“k折交叉验证。k最常用的取值是10,此时称为10折交叉验证；其他常用的k值有5、20等。

9、分析偏差和方差的含义。

答：

偏差是指模型预测值与真实值之间的平均差异。它反映了模型的系统误差，即模型在训练过程中可能对某些特征或模式的忽略或误解。

方差是指模型预测值在不同训练集上的变化程度。它反映了模型的随机误差，即模型对训练数据的敏感程度。

7、请简单介绍梯度下降算法，并给出梯度下降算法的步骤。

答：

梯度下降是一种用于优化模型参数的迭代算法，常用于最小化损失函数。在机器学习中，损失函数用于衡量模型的预测与真实值之间的差距，梯度下降通过不断更新模型参数，使得损失函数的值逐步减小，从而达到最优解。

梯度下降的基本思想是：沿着损失函数梯度的反方向（即下降最快的方向）更新参数，直到找到最小值或者达到预设的停止条件。

算法步骤：

1. 初始化参数：随机选择模型参数的初始值。
2. 计算梯度：计算损失函数相对于模型参数的梯度（偏导数），即梯度向量。
3. 更新参数：根据梯度更新模型参数。参数更新的规则是：

θ=θ-η∇f(θ)

4. 重复步骤 2 和 3：不断计算梯度并更新参数，直到损失函数的值收敛到一个最小值，或者达到最大迭代次数。

11、基于一些基本策略，可以利用二分类学习器解决多分类问题。多分类学习的基本思路是“拆解法”，将多分类任务拆为若干个二分类任务求解。最经典的拆分策略有一对一、一对多和多对多策略。如果给定数据集D中包含M个样本，对应有N个类别，请分析一对一、一对多和多对多策略。

答：

一对一策略：在一对一策略中，针对数据集中的每一对类别训练一个二分类器这意味着对于N个类别，训练N(N-1)/2个二分类器，其中N(N-1)/2是组合数，表示从N个类别中选出两个类别的方式。

一对多策略：在一对多策略中，对于每个类别，训练一个二分类器，该二分类器区分当前类别与所有其他类别，对于N个类别，训练N个二分类器。

多对多策略：多对多策略与一对一策略相似，也是在所有类别之间进行二分类任务，但在多对多策略中，每个二分类器区分两个类别，不考虑其他类别。

5、决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。特征选择的目的在于选择对训练数据能够分类的特征。特征选择的关键是其准则，常用的准则有哪些，请简单描述。

信息增益是衡量通过某个特征划分数据集后，数据的不确定性（熵）减少的程度

信息增益越大，表示该特征对数据集的分类效果越好。

增益率是为了克服信息增益偏向于取值较多特征的问题而引入的准则。它是在信

息增益的基础上考虑了特征的固有信息量，从而避免了对取值较多特征的偏好。

基尼指数是另一种常用于分类问题的特征选择准则，主要用于CART（分类与回

归树）算法中。基尼指数衡量的是数据集的不纯度，越小表示数据集越纯。

8、常用的决策树学习算法有ID3、C4.5和CART，介绍它们采用的特征选择准则是什么？

ID3算法：信息增益

C4.5算法：增益率

CART算法：基尼指数

9、简述决策树生成与决策树剪枝。

决策树生成：决策树生成是决策树学习过程的第一步，主要目的是从训练数据中

构建出一棵合适的决策树。决策树生成的核心过程是通过特征选择来递归地划分

数据集，直到满足某些停止准则为止。

决策树剪枝：决策树剪枝是生成完决策树后进行的优化过程，目的是通过移除不

必要的分支或节点来减少模型的复杂度，从而提高模型的泛化能力，减少过拟合

的风险。

决策树剪枝的基本策略有预剪枝和后剪枝，请简述并分析两种剪枝策略。

答：
预剪枝：在决策树生成过程中就决定是否停止进一步分裂，避免生成过深的树。

后剪枝：在生成完整的决策树后，使用某些策略剪去一些不必要的节点或分支,

简化模型。

5.假定一个单隐层的前馈神经网络，拥有m个输入神经元,n个输出神经元、q个隐层神经元，那么该神经网络中需要确定的连接权重参数有多少个？

答：需要确定的连接权重参数有（m*q+q*n）个。

常用来缓解BP网络的过拟合的策略有什么？

 答：

（1）早停：将数据分成训练集和验证集，训练集用来计算梯度、更新连接

权和阈值验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训

练，同时返回具有最小验证集误差的连接权和阈值。

（2）正则化：基本思想是在误差目标函数中增加一个用于描述网络复杂度的部

分，限制模型复杂度的增加。

请简述感知机模型，感知机的学习策略与学习算法。

 答：

感知机模型：由两层神经元组成，输入层和输出层，输入层接受外界信号，输出层输出结果，其只有一层功能神经元，学习能力有限，只能解决。

感知机学习策略：在假设空间中选取使损失函数最小的模型参数w和b。

感知机学习算法：随机梯度下降法。首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。在这个过程中一次随机选取一个误分类点使其梯度下降。

8. 误差逆传播（error BackPropagation，简称BP）算法是神经网络学习算法，简述使用BP算法训练多层前馈神经网络的工作过程。

答：

(1)训练开始前，随机初始化网格中所有连接权和阈值；

(2)前向传播，输入数据被送入神经网络的第一层，每一层的神经元计算其输入的加权和，然后通过激活函数转换，得到该层的输出，这个过程一直持续到输出层，产生预测结果；

(3)计算误差，使用损失函数计算网络预测输出与真实标签之间的误差；

(4)反向传播，令误差从输出层开始，逆向通过网络层传回输入层，在每一层上，计算误差相对于该层权重的梯度，使用链式法则计算每一层的梯度，一般是求导的方式；

(5)权重更新，新权重=旧权重-学习率x梯度；

(6)重复执行3-5步，直到达到预设的迭代次数或满足某些停止条件

9.简述标准BP算法与累积BP算法的区别。

答：

标准BP算法：每次仅针对一个训练样例更新连接权和阈值，即每个样本数据都会更新一次权重。这种方式导致参数更新非常频繁，而且对不同样本进行更新的效果可能出现"抵消"现象，因此，为了达到同样的累积误差极小点，标准BP算法往往需要更多次数的迭代。

累积BP算法：直接针对累积误差最小化，它在读取整个训练集一遍后才对参数进行更新，其参数更新低得多。，但在很多任务中，累计误差下降到一定程度后会进展缓慢，这种效应在训练集非常大时更明显。

试述SVM软间隔与SVM硬间隔的区别。

答：

硬间隔：硬间隔SVM假设数据是线性可分的，找到一个最大化间隔超平面，使得所有样本都分类正确。

软间隔：允许一些样本不满足式（6.28）的约束。

试述机器学习中L1正则化和L2正则化。

答：

LI和L2正则化都可以用于缓解过拟合现象。

L1正则化通过在损失函数中添加参数绝对值之和的惩罚项，可以使得一些参数变为0，从而实现特征选择，得到稀疏的权重矩阵。

L2正则化则是通过在损失函数中添加参数平方和的惩罚项来限制模型复杂度，它使得所有参数都较小但不为0，并且模型更加稳定。
